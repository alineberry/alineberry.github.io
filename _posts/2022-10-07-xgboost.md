---
title: "XGBoost"
date: 2022-10-07
tags: [machine learning]
excerpt: "XGBoost Derivation +"
mathjax: true
author_profile: true
permalink: /notes/pca
is_note: true
---

Consider a dataset of $$n$$ observations $$ \mathcal{D} = \{ (x_i, y_i) \}_{i=1}^n $$ where  $$ x_i \in \mathbb{R}^d $$ are the observed features and $$ y_i \in \mathbb{R} $$ are the targets.

An additive tree model produces a prediction by summing the predictions of $$K$$ trees:

$$
\hat{y_i} = \sum_{k=1}^K f_k(x_i)
\\
f(x) = w_{q(x)} \quad q : \mathbb{R}^T \rightarrow T \quad w \in \mathbb{R}^T
$$

Where $$w$$ is a vector of $$T$$ leaf scores and $$q(x)$$ maps data to a leaf node index.

The objective used to learn the model consists of a loss function that measures the difference between predictions $$\hat{y_i}$$ and targets $$y_i$$ combined with a regularization term $$\Omega$$ that penalizes model complexity:

$$
\mathcal{L} = \sum_i l(y_i, \hat{y_i}) + \sum_k \Omega(f_k)
$$

The regularization term for a single tree is defined as:

$$
\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
$$

The model is learned in a greedy, additive fashion, learning a new tree at each iteration. The prediction and corresponding objective at iteration $$t$$ is as follows:

$$
\begin{align*}
\hat{y}^{(t)} &= \hat{y}_i^{(t-1)} + f_t(x_i)
\\
\mathcal{L}^{(t)} &= \sum_i l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
\end{align*}
$$

The objective is approximated with 2nd order Taylor expansion:

$$
\mathcal{L}^{(t)} \simeq \sum_i \Bigl[ l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \Bigr] + \Omega(f_k) \\

g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}} 
\quad
h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}
$$

Remove constants with respect to the tree being learned $$f_t$$:

$$
\tilde{\mathcal{L}}^{(t)} = \sum_i \Bigl[ g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \Bigr] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2
$$

Next, let's define $$ I_j = \{ i \lvert q(x_i) = j \} $$ as the set of instance indices belonging to leaf node $$j$$ and rearrange the objective to loop over leaf nodes instead of instances:

$$
\tilde{\mathcal{L}}^{(t)} = \sum_j \Bigl[ 
    (\sum_{i \in I_j} g_i) w_j
    + \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) w_j^2
\Bigr] + \gamma T
$$

From here, we want to solve for optimal leaf scores $$w_j$$ that minimize the objective. The loss contributed by a particular leaf $$j$$ is:

$$
\mathcal{L}_j = (\sum_{i \in I_j} g_i) w_j
+ \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) w_j^2
$$

And the optimal leaf score for that leaf is the value that minimizes that leaf's contribution to the overall objective $$ w_j^* = \arg \min_{w_j} \: \mathcal{L}_j $$. 

Set derivative equal to zero and solve for $$w_j^*$$

$$
\frac{\partial \mathcal{L}_j}{\partial w_j^*} = 0 = 
\sum_{i \in I_j} g_i + (\sum_{i \in I_j} h_i + \lambda) w_j^*
\\
w_j^* = - \frac{\sum_{i \in I_j} g_i}
               {\sum_{i \in I_j} h_i + \lambda}
$$

We can plug in $$w_j^*$$ into the objective to obtain a scoring function to measure the quality of a tree. This score is similar to an impurity measure, but more generalized:

$$
\begin{align*}

\tilde{\mathcal{L}}^{(t)} &= \sum_j \Bigl[ 
    (\sum_{i \in I_j} g_i) w_j
    + \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) w_j^2
\Bigr] + \gamma T
\\
&= \sum_j \Bigl[ 
    (\sum_{i \in I_j} g_i) \Biggl(- \frac{\sum_{i \in I_j} g_i}
               {\sum_{i \in I_j} h_i + \lambda} \Biggr)
    + \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) 
    {\Biggl(- \frac{\sum_{i \in I_j} g_i}
               {\sum_{i \in I_j} h_i + \lambda} \Biggr)}^2
\Bigr] + \gamma T
\\
&= \sum_j \Bigl[ 
    -\frac{(\sum_{i \in I_j} g_i)^2}
          {\sum_{i \in I_j} h_i + \lambda}
    + \frac{1}{2} \frac{(\sum_{i \in I_j} g_i)^2}
          {\sum_{i \in I_j} h_i + \lambda}
\Bigr] + \gamma T
\\
&= - \frac{1}{2} \sum_{j=1}^T
\frac{(\sum_{i \in I_j} g_i)^2}
     {\sum_{i \in I_j} h_i + \lambda}
+ \lambda T

\end{align*}
$$

It is impossible to enumerate all possible tree structures, so we use a greedy algorithm that builds tree $$f_t$$ by iteratively adding branches (i.e., chooses a feature and a corresponding split value) according to the magnitude of loss reduction:

$$
\mathcal{L}_{split} = \mathcal{L}_{left} + \mathcal{L}_{right} - \mathcal{L}_{root}
$$